{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ab993a",
   "metadata": {},
   "source": [
    "# Detecting Topic Merges and Splits in Dynamic Political Conversations\n",
    "\n",
    "Cláudia Oliveira \n",
    "\n",
    "Supervisor - Prof. Dr. Álvaro Figueira\n",
    "\n",
    "Faculty of Science, University of Porto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3becb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun GPE\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f594b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"datasets/tweets.csv\", encoding=\"utf-8\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo spaCy (inglês)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Processar todos os tweets eficientemente\n",
    "docs = list(nlp.pipe(tweets[\"text\"].astype(str), batch_size=50))\n",
    "\n",
    "# Extrair todas as entidades únicas\n",
    "entidades_unicas = set()\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        entidades_unicas.add(ent.text)\n",
    "\n",
    "# Mostrar as entidades únicas\n",
    "print(\"Número de entidades únicas:\", len(entidades_unicas))\n",
    "print(entidades_unicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a2430",
   "metadata": {},
   "source": [
    "230 000 entitites"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
